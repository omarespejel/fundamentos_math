{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase_4_1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRsJ1xwwde4K",
        "colab_type": "code",
        "outputId": "679bfc7c-6cd9-427a-b8e5-62374b9cfe5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 28kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 33.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 40.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Collecting gast==0.2.2 (from tensorflow-gpu==2.0.0)\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.0.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=47b5f6941d0c6a22c54e1fa9b618695bb5ae8d32deffb1c541e478743d81f160\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorboard<1.15.0,>=1.14.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.14.0 has requirement tensorflow-estimator<1.15.0rc0,>=1.14.0rc0, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, gast, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.14.0\n",
            "    Uninstalling tensorboard-1.14.0:\n",
            "      Successfully uninstalled tensorboard-1.14.0\n",
            "  Found existing installation: tensorflow-estimator 1.14.0\n",
            "    Uninstalling tensorflow-estimator-1.14.0:\n",
            "      Successfully uninstalled tensorflow-estimator-1.14.0\n",
            "  Found existing installation: gast 0.3.2\n",
            "    Uninstalling gast-0.3.2:\n",
            "      Successfully uninstalled gast-0.3.2\n",
            "Successfully installed gast-0.2.2 tensorboard-2.0.0 tensorflow-estimator-2.0.0 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTOj4tZ4mGkl",
        "colab_type": "code",
        "outputId": "c5695801-2b78-4002-89d1-27ec64bdbdee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1e2l7sT75p",
        "colab_type": "text"
      },
      "source": [
        "Observamos que estamos operando con la versión 2.0.0 de tensorflow, liberada el 30 de septiembre de 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFmMxHyhlghJ",
        "colab_type": "text"
      },
      "source": [
        "Veamos si tenemos los conocimientos matemáticos necesarios para hacer state-of-the-art deep learning. Esta lección se centrará en lo estrictamente necesario para hacer deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJfB14OXljhV",
        "colab_type": "text"
      },
      "source": [
        "Definamos de manera sencilla lo que es un grafo. Las redes en deep learning no son más que larguísimas funciones que ante la presencia de decenas de capas se pueden volver terriblemente tediosas de representar. Los grafos nos permiten organizar nuestro pensamiento acerca de las funciones que estamos construyendo, y de hecho mucha investigación actual usa la representación gráfica para desarrollar nuevas y mejores estructuras como lo son las resnet y las efficientnets (más adelante las estudiaremos). Siendo más precisios, las redes neuronales son un tipo particular de grafos, son grafos computacionales. En otras palabras, es más sencillo representar las arquitecturas de nuestras redes por medio de grafos que de por medio de notación matemática; funciones insertadas en funciones insertadas en funciones...\n",
        "\n",
        "Los grafos computacionales pueden lucir como cualquiera de las siguientes dos arquitecturas, la primera representa la función $(x+y)*z$ y la segunda una arquitectura resnet de 152 capas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_taMyQD_C3n",
        "colab_type": "text"
      },
      "source": [
        "Ahora, para entender las matemáticas necesarias entendamos primero a lo que nos referimos con una red neuronal y en dónde se encuentra cada función que definiremos más adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgTwQXyJDs_4",
        "colab_type": "text"
      },
      "source": [
        "*Nota: a las redes neuronales también se les llama \"artificial neural networks\" (ANN) y \"perceptrones de múltiples capas\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pee_mpV7_7vi",
        "colab_type": "text"
      },
      "source": [
        "La siguiente imagen (tomada de la clase CS231n Convolutional Neural Networks for Visual Recognition de Stanford) muestra dos arquitecturas diferentes de red neuronales con capas fully connected (completamente conectadas, en español). En el caso de la izquierda tenemos una red neuronal que cuenta con datos que tienen tres paramétros de entrada. Si nuestros ejemplos son casas, entonces los parámetros pueden ser el número de metros cuadrados que ocupan, la localización y si tienen o no patio). A la primera capa se le llama input layer (capa de entrada, en español). El número de parámetros de la segunda capa es definido por nosotros y a cada parámetro le llamamos unidad computacional o neurona, como se prefiera. En este caso, la arquitectura de la izquierda cuenta con cuatro neuronas. A esta la capa le llamamos hidden layer (capa oculta, español). Finalmente, la output layer (capa de salida) cuenta en este caso con dos neuronas y se utiliza para representar el valor que se le da a cada clase en que podemos clasificar (en el caso de la clasificación de imágenes te indica, por ejemplo, si tu imagen es más probable que sea un gato o un perro; una neurona te da el puntaje con respecto a la clase \"perro\" y la otra con respecto a la clase \"gato\".\n",
        "\n",
        "La arquitectura de la derecha es similar en cuanto que cuenta con capas fully connected pero difiere en que cuenta con dos hidden layer (podemos poner las que queramos, siempre y cuando tomemos en cuenta otros factores con los que lidiaremos más adelante) y con una única neurona en el output layer (podría ser el valor predicho del precio de una acción).\n",
        "\n",
        "Llamamos a la primera arquitectura como una red neuronal de dos layers y a la segunda como red neuronal de tres layers. Las hidden y output layers definen el número de layers en el nombre de la red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdXtLKOIEq51",
        "colab_type": "text"
      },
      "source": [
        "Dentro de cada neurona suceden, al menos, dos operaciones. Primero, se computa la affine function (función afín, en español): poducto punto entre un vector de pesos y un vector $\\vec{x}$. La affine function no es más que una función linear y depende del tipo de red neuronal que queramos construir (por ejemplo, la affine function puede ser una operación de convolución para clasificación de imágenes). Al producto de la affine function sigue una función de no linearidad que suele llevar a los valores negativos a cero. Más adelante entraremos en detalles. Al terminar ambas operaciones decimos que \"activamos\" la neurona. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oquReNmHQXN1",
        "colab_type": "text"
      },
      "source": [
        "Al considerar todas las neuronas en las hidden layers estamos multiplicando una matriz por otra matriz (tensores de rango dos) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g4NteqOKSaO",
        "colab_type": "text"
      },
      "source": [
        "En esta sesión clasificamos la famosa base de datos MNIST para hacer reconocimiento de números en imágenes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0kIRlSZKn1W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q3CmHWYMZrO",
        "colab_type": "text"
      },
      "source": [
        "Por el momento no hagamos caso a nuestros sets de evaluación. En nuestro set de entrenamiento tenemos 6,000 imágenes de 28 x 28 pixeles. Las imágenes están en blanco y negro por lo que no tienen una dimensión RGB. Cada una de las 6,000 imágenes representa un número entre 0 y 9, 10 número en total, y dicha información se encuentra en `y_train`, de forma 60,000 x 1. Podemos dar `x_train.shape, y_train.shape` para conocer esta información. Notar que los datos están en formato numpy, sin embargo al hacer operaciones de tensorflow 2.0 sobre ellos se convierten automáticamente en tensores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaJpS3Hdmb6M",
        "colab_type": "text"
      },
      "source": [
        "Aplanamos y convertimos a float nuestras imágenes para que podamos operar sobre ellas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrzB5RN8nH22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.reshape(x_train, (60000,-1))\n",
        "t = tf.dtypes.cast(t, dtype = tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrckfZpPaGK",
        "colab_type": "text"
      },
      "source": [
        "Ahora nuestro tensor `t` tiene la forma `TensorShape([6000, 784])` donde el 784 viene de multiplicar 28 x 28."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzM1L5kwni3g",
        "colab_type": "text"
      },
      "source": [
        "Guardemos algunas características relevantes de nuestra imagen. `n` será el número de imágenes que tenemos y `m` nuestro número de parámetros iniciales. Además, nh será el número de neuronas que tendremos en nuestra primera capa, ya hablaremos más tarde de eso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbjS1BJ_nU9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n, m = t.shape\n",
        "nh1 = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lIhb566PCgx",
        "colab_type": "text"
      },
      "source": [
        "`n` es igual a 60,000, el número de imágenes; m es el número de parámetros de nuestro modelo, 784, un valor por cada pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxnxLw5loIgi",
        "colab_type": "text"
      },
      "source": [
        "Introduzcamos una estructura de datos que multiplicará a nuestras imágenes y la llamaremos weights (pesos , en español). Definamos las dimensiones que debe tener nuestro tensor de pesos: \n",
        "\n",
        "\n",
        "\n",
        "1.   Nuestros weights harán una multiplación de matrices con nuestras imágenes.\n",
        "2.   Queremos que tenga en la otra dimensión el número de neuronas que definimos para nuestra primera capa, `nh1`.\n",
        "\n",
        "Nuestros weights entonces tendrán la forma (m, nh). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVhQpYN2oHLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = tf.random.normal((m, nh1))*0.0001 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0pmB8oYsFgv",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, queremos que nuestros pesos tengan una característica muy particular. En cualquier función existen constantes y variables. En deep learning se suele tratar a nuestros datos (imágenes en este caso) como constantes; el valor de nuestros pixeles no cambiará en ningún momento. Nuestras variables serán nuestros weights; los queremos de forma que sean lo más certeros posibles a la hora de predecir lo que sea que queremos predecir. En otras palabras, nosotros operaremos sobre nuestros pesos, los variaremos hasta que encontremos la combinación de pesos que mejor hagan mapping entre nuestros datos y lo que queremos predecir/clasificar. Específicamente, queremos saber como cambia nuestra función de pérdida cada vez que variemos nuestros pesos.\n",
        "\n",
        "El método `tf.GradientTape()` nos permite registrar los gradientes de una función con respecto a las variables que la integran. En otras, palabras, cuando activamos `tf.GradientTape()` con respecto a una función estamos indicando a tensorflow que eventualmente vamos a requerir los gradientes de dicha función. \n",
        "\n",
        "Quedará más claro conforme vayamos construyendo nuestra red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoXw7KvFtord",
        "colab_type": "text"
      },
      "source": [
        "*Nota: el recurso de computar los gradientes es más conocido en la librería de pytorch y su método `autograd`. Para los que prefieran una explicación más técnica: el autograd es un \"[...] motor para calcular derivadas (producto jacobiano-vector para ser más precisos). Registra un grafo de todas las operaciones realizadas en un tensor con gradiente habilitado y crea un gráfico acíclico llamado gráfico computacional dinámico. Las hojas de este grafo son tensores de entrada y las raíces son tensores de salida. Los gradientes se calculan trazando el grafo desde la raíz hasta la hoja y multiplicando cada gradiente en el camino usando la regla de la cadena.\" - [Vaibhav Kumar](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muJ8lEvUwJS7",
        "colab_type": "text"
      },
      "source": [
        "Para entender mejor el autograd hagamos lo siguiente. Considera la función $$y(x) = x^2,$$ de nuestra clase de cálculo 1 sabemos que $$y'(x) = 2*x,$$ entonces si $x = 5$, $$y'(5) = 2 * 5 = 10.$$ \n",
        "\n",
        "Veamos como autograd nos puede dar el mismo resultado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo12yAl9-5mX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(5.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ot7jBeAyv5A",
        "colab_type": "text"
      },
      "source": [
        "Definimos nuestra función $y(x) = x^2$ y activamos el `tf.GradientTape()` para requerir los gradientes, en este caso la derivada con respecto a x. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbrshg9BzPJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUlXsCt0zXmN",
        "colab_type": "text"
      },
      "source": [
        "Ahora usemos la propiedad `gradient()` de nuestro tensor GradientTape, `tape`, para obtener el gradiente de `y` con respecto a `x` y así para conocer el valor de la derivada. *Nota: usemos numpy para imprimir nuestro resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWPSGZWF_e1v",
        "colab_type": "code",
        "outputId": "b97640ba-9c83-4d41-8163-8b836b1e341a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "grad = tape.gradient(y,x)\n",
        "grad.numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUdJNYJvzrFL",
        "colab_type": "text"
      },
      "source": [
        "Nos da como resultado 10. Justo lo que buscabamos. Este mismo procedimiento es el que seguiremos más adelante pero con varias funciones compuestas. Como vemos, nada complicado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF9GaqgHh2yB",
        "colab_type": "text"
      },
      "source": [
        "Desarrollemos ahora nuestro \"array programming\". En la programación científica tenemos una preocupación especial por llevar a acabo operaciones en un gran número de valores de manera paralela. El array programming se ve de manera natural en lenguajes como APL, R y Matlab, y en frameworks para Python como numpy o pytorch. Sin array programming es imposible resolver los problemas computacionales que el deep learning demanda de nosotros, punto.\n",
        "\n",
        "Dicho esto, ahora dictemos que nuestro tensor `x` no sea de rango cero (escalar) sino de rango uno (vector). Como con el caso anterior, queremos que la misma función $x^2$ se aplique para llegar al resultado de `y` y poesteriormente obtener el gradiente con respecto a nuestros valores en `x`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnL-rmrokJHO",
        "colab_type": "code",
        "outputId": "3de133c7-5268-427e-a10c-08d294f2878e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x = tf.Variable([5.,4.,3.,2.,1.,0.])\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "tape.gradient(y,x).numpy()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.,  8.,  6.,  4.,  2.,  0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_rQnA6B4R6",
        "colab_type": "text"
      },
      "source": [
        "Ahora que estamos más familiarizados con el array programming, introduzcamos dos funciones clave para el deep learning: la función de pérdida y la función ReLU. Ninguna representa mayor dificultad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbP5Fm5sCX5j",
        "colab_type": "text"
      },
      "source": [
        "Existen muchos tipos diferentes tanto de funciones de pérdida como de funciones de no-linearidad. Aquí utilizamos como pérdida la función de \"log softmax\": \n",
        "$$\\operatorname{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}},$$ o en otra notación:\n",
        "$$\\operatorname{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
        "\n",
        "<!-- donde llamaremos a $Y_i$ como \"output\" (producto, en español) pues será el producto final de nuestra serie de funciones que representan nuestra red neuronal; y a $\\hat{Y_i}$ le llamaremos \"target\" (objetivo, en español) pues es el valor al que aspiramos que nuestro output sea igual. -->\n",
        "En la práctia aplicaremos logaritmo a nuestra softmax. No se preocupen por la notación matemática, como dice Jeremy Howard, está forma de escribir es bastante de flojera. Ya veremos como la función requiere una única línea de código gracias al array programming. \n",
        "\n",
        "Como función de no linearidad utilizaremos la Rectified Linear Unit (ReLU): \n",
        "$$\\operatorname{ReLU(x)} = \\begin{cases}\n",
        "    x & \\text{si } x > 0, \\\\\n",
        "    0 & \\text{de otra forma}.\n",
        "\\end{cases}$$\n",
        "\n",
        "Para ambas funciones $x$ será comunmente un tensor de rango dos y gracias al array programming nuestras funciones podrán ser aplicadas simultáneamente a cada elemento dentro de nuestro tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jas55fj-TceB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): \n",
        "  return x - tf.math.log(tf.math.reduce_sum((tf.math.exp(x)), axis=-1, keepdims=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWBjt-RglVlJ",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow tiene su propia implementación de la función de log softmax, `tf.nn.log_softmax`, y podemos comparar su resultado con el de nuestra `log_softmax` y veremos que el resultado es el mismo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoDkK9iFCVxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "  return tf.math.maximum(x, 0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GBu8VwlnAAV",
        "colab_type": "text"
      },
      "source": [
        "De la misma manera, tensorflow cueta con la función `tf.nn.relu` y el resultado es el mismo al de nuestra función `relu`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZZGoWpCuldQ",
        "colab_type": "text"
      },
      "source": [
        "Estamos listos para multiplicar nuestros pesos por nuestras imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11eGcLutMy4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_2 = tf.linalg.matmul(t, w1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnHb4wnTbJu",
        "colab_type": "code",
        "outputId": "eba035b2-85cd-48aa-defa-80b5f469083d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "t_2.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaiEyBu8u8ed",
        "colab_type": "text"
      },
      "source": [
        "Ahora tenemos un tensor con dimensiones `TensorShape([60000, 100])`. Es decir, tenemos un tensor rango 1 por cada una de nuestras 60,000 imágenes y cada imagen ahora tiene propiedades en cada una de nuestras 100 neuronas definidas para nuestra capa 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eb_JG0c0Yk2",
        "colab_type": "text"
      },
      "source": [
        "Ahora apliquemos nuestra función `relu` a `t_2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFm8R_wC1Hie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_2_relu = relu(t_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MFEgB_UprrB",
        "colab_type": "text"
      },
      "source": [
        "Notar que al aplicar una función no estamos alterando la forma de nuestros tensores pues la misma función se aplica a cada elemento del tensor. La forma sigue siendo `TensorShape([60000, 100])`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXIVapiFp91e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_2_softmax = log_softmax(t_2_relu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfpZ5fCv4zKU",
        "colab_type": "text"
      },
      "source": [
        "Ahora vemos que una red neuronal no es más que una función integrada por múltiples funciones; funciones compuestas. ¿Dónde está el proceso de entrenamiento? Este proceso se hace gracias al algoritmo llamado \"backpropagation\". Nos queda para la parte 2."
      ]
    }
  ]
}