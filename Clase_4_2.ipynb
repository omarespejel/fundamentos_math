{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase_4_1 (3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRsJ1xwwde4K",
        "colab_type": "code",
        "outputId": "e3b9269e-ea8b-4463-dd36-ef1cb32c7295",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 710
        }
      },
      "source": [
        "!pip install tensorflow-gpu==2.0.0"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/44/47f0722aea081697143fbcf5d2aa60d1aee4aaacb5869aee2b568974777b/tensorflow_gpu-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (380.8MB)\n",
            "\u001b[K     |████████████████████████████████| 380.8MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.1.7)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.16.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.1.0)\n",
            "Collecting tensorboard<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 35.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.15.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.11.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.8.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (3.7.1)\n",
            "Collecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow-gpu==2.0.0)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0) (0.33.6)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (41.2.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.0) (0.16.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0) (2.8.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 2.0.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0rc3 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 2.0.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-estimator 1.15.1\n",
            "    Uninstalling tensorflow-estimator-1.15.1:\n",
            "      Successfully uninstalled tensorflow-estimator-1.15.1\n",
            "Successfully installed tensorboard-2.0.0 tensorflow-estimator-2.0.0 tensorflow-gpu-2.0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZTOj4tZ4mGkl",
        "colab_type": "code",
        "outputId": "405bcecb-644a-4765-c44a-cc4228969c59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.0.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8u1e2l7sT75p",
        "colab_type": "text"
      },
      "source": [
        "Observamos que estamos operando con la versión 2.0.0 de tensorflow, liberada el 30 de septiembre de 2019."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fFmMxHyhlghJ",
        "colab_type": "text"
      },
      "source": [
        "Veamos si tenemos los conocimientos matemáticos necesarios para hacer state-of-the-art deep learning. Esta lección se centrará en lo estrictamente necesario para hacer deep learning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJfB14OXljhV",
        "colab_type": "text"
      },
      "source": [
        "Definamos de manera sencilla lo que es un grafo. Las redes en deep learning no son más que larguísimas funciones que ante la presencia de decenas de capas se pueden volver terriblemente tediosas de representar. Los grafos nos permiten organizar nuestro pensamiento acerca de las funciones que estamos construyendo, y de hecho mucha investigación actual usa la representación gráfica para desarrollar nuevas y mejores estructuras como lo son las resnet y las efficientnets (más adelante las estudiaremos). Siendo más precisios, las redes neuronales son un tipo particular de grafos, son grafos computacionales. En otras palabras, es más sencillo representar las arquitecturas de nuestras redes por medio de grafos que de por medio de notación matemática; funciones insertadas en funciones insertadas en funciones...\n",
        "\n",
        "Los grafos computacionales pueden lucir como cualquiera de las siguientes dos arquitecturas, la primera representa la función $(x+y)*z$ y la segunda una arquitectura resnet de 152 capas."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_taMyQD_C3n",
        "colab_type": "text"
      },
      "source": [
        "Ahora, para entender las matemáticas necesarias entendamos primero a lo que nos referimos con una red neuronal y en dónde se encuentra cada función que definiremos más adelante."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgTwQXyJDs_4",
        "colab_type": "text"
      },
      "source": [
        "*Nota: a las redes neuronales también se les llama \"artificial neural networks\" (ANN) y \"perceptrones de múltiples capas\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pee_mpV7_7vi",
        "colab_type": "text"
      },
      "source": [
        "La siguiente imagen (tomada de la clase CS231n Convolutional Neural Networks for Visual Recognition de Stanford) muestra dos arquitecturas diferentes de red neuronales con capas fully connected (completamente conectadas, en español). En el caso de la izquierda tenemos una red neuronal que cuenta con datos que tienen tres paramétros de entrada. Si nuestros ejemplos son casas, entonces los parámetros pueden ser el número de metros cuadrados que ocupan, la localización y si tienen o no patio). A la primera capa se le llama input layer (capa de entrada, en español). El número de parámetros de la segunda capa es definido por nosotros y a cada parámetro le llamamos unidad computacional o neurona, como se prefiera. En este caso, la arquitectura de la izquierda cuenta con cuatro neuronas. A esta la capa le llamamos hidden layer (capa oculta, español). Finalmente, la output layer (capa de salida) cuenta en este caso con dos neuronas y se utiliza para representar el valor que se le da a cada clase en que podemos clasificar (en el caso de la clasificación de imágenes te indica, por ejemplo, si tu imagen es más probable que sea un gato o un perro; una neurona te da el puntaje con respecto a la clase \"perro\" y la otra con respecto a la clase \"gato\".\n",
        "\n",
        "La arquitectura de la derecha es similar en cuanto que cuenta con capas fully connected pero difiere en que cuenta con dos hidden layer (podemos poner las que queramos, siempre y cuando tomemos en cuenta otros factores con los que lidiaremos más adelante) y con una única neurona en el output layer (podría ser el valor predicho del precio de una acción).\n",
        "\n",
        "Llamamos a la primera arquitectura como una red neuronal de dos layers y a la segunda como red neuronal de tres layers. Las hidden y output layers definen el número de layers en el nombre de la red."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdXtLKOIEq51",
        "colab_type": "text"
      },
      "source": [
        "Dentro de cada neurona suceden, al menos, dos operaciones. Primero, se computa la affine function (función afín, en español): poducto punto entre un vector de pesos y un vector $\\vec{x}$. La affine function no es más que una función linear y depende del tipo de red neuronal que queramos construir (por ejemplo, la affine function puede ser una operación de convolución para clasificación de imágenes). Al producto de la affine function sigue una función de no linearidad que suele llevar a los valores negativos a cero. Más adelante entraremos en detalles. Al terminar ambas operaciones decimos que \"activamos\" la neurona. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oquReNmHQXN1",
        "colab_type": "text"
      },
      "source": [
        "Al considerar todas las neuronas en las hidden layers estamos multiplicando una matriz por otra matriz (tensores de rango dos) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2g4NteqOKSaO",
        "colab_type": "text"
      },
      "source": [
        "En esta sesión clasificamos la famosa base de datos MNIST para hacer reconocimiento de números en imágenes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0kIRlSZKn1W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "d527a023-ac54-438a-f79c-302b0bdebead"
      },
      "source": [
        "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1q3CmHWYMZrO",
        "colab_type": "text"
      },
      "source": [
        "Por el momento no hagamos caso a nuestros sets de evaluación. En nuestro set de entrenamiento tenemos 6,000 imágenes de 28 x 28 pixeles. Las imágenes están en blanco y negro por lo que no tienen una dimensión RGB. Cada una de las 6,000 imágenes representa un número entre 0 y 9, 10 número en total, y dicha información se encuentra en `y_train`, de forma 60,000 x 1. Podemos dar `x_train.shape, y_train.shape` para conocer esta información. Notar que los datos están en formato numpy, sin embargo al hacer operaciones de tensorflow 2.0 sobre ellos se convierten automáticamente en tensores."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaJpS3Hdmb6M",
        "colab_type": "text"
      },
      "source": [
        "Aplanamos y convertimos a float nuestras imágenes para que podamos operar sobre ellas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MrzB5RN8nH22",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = tf.reshape(x_train, (60000,-1))\n",
        "t = tf.dtypes.cast(t, dtype = tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFrckfZpPaGK",
        "colab_type": "text"
      },
      "source": [
        "Ahora nuestro tensor `t` tiene la forma `TensorShape([6000, 784])` donde el 784 viene de multiplicar 28 x 28."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GzM1L5kwni3g",
        "colab_type": "text"
      },
      "source": [
        "Guardemos algunas características relevantes de nuestra imagen. `n` será el número de imágenes que tenemos y `m` nuestro número de parámetros iniciales. Además, nh será el número de neuronas que tendremos en nuestra primera capa, ya hablaremos más tarde de eso."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WbjS1BJ_nU9c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n, m = t.shape\n",
        "nh1 = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lIhb566PCgx",
        "colab_type": "text"
      },
      "source": [
        "`n` es igual a 60,000, el número de imágenes; m es el número de parámetros de nuestro modelo, 784, un valor por cada pixel."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xxnxLw5loIgi",
        "colab_type": "text"
      },
      "source": [
        "Introduzcamos una estructura de datos que multiplicará a nuestras imágenes y la llamaremos weights (pesos , en español). Definamos las dimensiones que debe tener nuestro tensor de pesos: \n",
        "\n",
        "\n",
        "\n",
        "1.   Nuestros weights harán una multiplación de matrices con nuestras imágenes.\n",
        "2.   Queremos que tenga en la otra dimensión el número de neuronas que definimos para nuestra primera capa, `nh1`.\n",
        "\n",
        "Nuestros weights entonces tendrán la forma (m, nh). \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVhQpYN2oHLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = tf.random.normal((m, nh1))*0.0001 "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0pmB8oYsFgv",
        "colab_type": "text"
      },
      "source": [
        "Finalmente, queremos que nuestros pesos tengan una característica muy particular. En cualquier función existen constantes y variables. En deep learning se suele tratar a nuestros datos (imágenes en este caso) como constantes; el valor de nuestros pixeles no cambiará en ningún momento. Nuestras variables serán nuestros weights; los queremos de forma que sean lo más certeros posibles a la hora de predecir lo que sea que queremos predecir. En otras palabras, nosotros operaremos sobre nuestros pesos, los variaremos hasta que encontremos la combinación de pesos que mejor hagan mapping entre nuestros datos y lo que queremos predecir/clasificar. Específicamente, queremos saber como cambia nuestra función de pérdida cada vez que variemos nuestros pesos.\n",
        "\n",
        "El método `tf.GradientTape()` nos permite registrar los gradientes de una función con respecto a las variables que la integran. En otras, palabras, cuando activamos `tf.GradientTape()` con respecto a una función estamos indicando a tensorflow que eventualmente vamos a requerir los gradientes de dicha función. \n",
        "\n",
        "Quedará más claro conforme vayamos construyendo nuestra red neuronal."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FoXw7KvFtord",
        "colab_type": "text"
      },
      "source": [
        "*Nota: el recurso de computar los gradientes es más conocido en la librería de pytorch y su método `autograd`. Para los que prefieran una explicación más técnica: el autograd es un \"[...] motor para calcular derivadas (producto jacobiano-vector para ser más precisos). Registra un grafo de todas las operaciones realizadas en un tensor con gradiente habilitado y crea un gráfico acíclico llamado gráfico computacional dinámico. Las hojas de este grafo son tensores de entrada y las raíces son tensores de salida. Los gradientes se calculan trazando el grafo desde la raíz hasta la hoja y multiplicando cada gradiente en el camino usando la regla de la cadena.\" - [Vaibhav Kumar](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muJ8lEvUwJS7",
        "colab_type": "text"
      },
      "source": [
        "Para entender mejor el autograd hagamos lo siguiente. Considera la función $$y(x) = x^2,$$ de nuestra clase de cálculo 1 sabemos que $$y'(x) = 2*x,$$ entonces si $x = 5$, $$y'(5) = 2 * 5 = 10.$$ \n",
        "\n",
        "Veamos como autograd nos puede dar el mismo resultado. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wo12yAl9-5mX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.Variable(5.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ot7jBeAyv5A",
        "colab_type": "text"
      },
      "source": [
        "Definimos nuestra función $y(x) = x^2$ y activamos el `tf.GradientTape()` para requerir los gradientes, en este caso la derivada con respecto a x. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbrshg9BzPJU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with tf.GradientTape() as tape:\n",
        "  y = x**2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUlXsCt0zXmN",
        "colab_type": "text"
      },
      "source": [
        "Ahora usemos la propiedad `gradient()` de nuestro tensor GradientTape, `tape`, para obtener el gradiente de `y` con respecto a `x` y así para conocer el valor de la derivada. *Nota: usemos numpy para imprimir nuestro resultado."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWPSGZWF_e1v",
        "colab_type": "code",
        "outputId": "5fad8b37-e934-4c86-9e78-4d458dfb5af4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "grad = tape.gradient(y,x)\n",
        "grad.numpy()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUdJNYJvzrFL",
        "colab_type": "text"
      },
      "source": [
        "Nos da como resultado 10. Justo lo que buscabamos. Este mismo procedimiento es el que seguiremos más adelante pero con varias funciones compuestas. Como vemos, nada complicado."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF9GaqgHh2yB",
        "colab_type": "text"
      },
      "source": [
        "Desarrollemos ahora nuestro \"array programming\". En la programación científica tenemos una preocupación especial por llevar a acabo operaciones en un gran número de valores de manera paralela. El array programming se ve de manera natural en lenguajes como APL, R y Matlab, y en frameworks para Python como numpy o pytorch. Sin array programming es imposible resolver los problemas computacionales que el deep learning demanda de nosotros, punto.\n",
        "\n",
        "Dicho esto, ahora dictemos que nuestro tensor `x` no sea de rango cero (escalar) sino de rango uno (vector). Como con el caso anterior, queremos que la misma función $x^2$ se aplique para llegar al resultado de `y` y poesteriormente obtener el gradiente con respecto a nuestros valores en `x`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bnL-rmrokJHO",
        "colab_type": "code",
        "outputId": "d142de44-2c45-4194-84e8-bd05ab9aca89",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = tf.Variable([5.,4.,3.,2.,1.,0.])\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "tape.gradient(y,x).numpy()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([10.,  8.,  6.,  4.,  2.,  0.], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZ_rQnA6B4R6",
        "colab_type": "text"
      },
      "source": [
        "Ahora que estamos más familiarizados con el array programming, introduzcamos dos funciones clave para el deep learning: la función de pérdida y la función ReLU. Ninguna representa mayor dificultad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbP5Fm5sCX5j",
        "colab_type": "text"
      },
      "source": [
        "Existen muchos tipos diferentes tanto de funciones de pérdida como de funciones de no-linearidad. Aquí utilizamos como pérdida la función de \"log softmax\": \n",
        "$$\\operatorname{softmax(x)}_{i} = \\frac{e^{x_{i}}}{\\sum_{0 \\leq j \\leq n-1} e^{x_{j}}},$$ o en otra notación:\n",
        "$$\\operatorname{softmax(x)}_{i} = \\frac{e^{x_{i}}}{e^{x_{0}} + e^{x_{1}} + \\cdots + e^{x_{n-1}}}$$\n",
        "\n",
        "<!-- donde llamaremos a $Y_i$ como \"output\" (producto, en español) pues será el producto final de nuestra serie de funciones que representan nuestra red neuronal; y a $\\hat{Y_i}$ le llamaremos \"target\" (objetivo, en español) pues es el valor al que aspiramos que nuestro output sea igual. -->\n",
        "En la práctia aplicaremos logaritmo a nuestra softmax. No se preocupen por la notación matemática, como dice Jeremy Howard, está forma de escribir es bastante de flojera. Ya veremos como la función requiere una única línea de código gracias al array programming. \n",
        "\n",
        "Como función de no linearidad utilizaremos la Rectified Linear Unit (ReLU): \n",
        "$$\\operatorname{ReLU(x)} = \\begin{cases}\n",
        "    x & \\text{si } x > 0, \\\\\n",
        "    0 & \\text{de otra forma}.\n",
        "\\end{cases}$$\n",
        "\n",
        "Para ambas funciones $x$ será comunmente un tensor de rango dos y gracias al array programming nuestras funciones podrán ser aplicadas simultáneamente a cada elemento dentro de nuestro tensor."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jas55fj-TceB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def log_softmax(x): \n",
        "  return x - tf.math.log(tf.math.reduce_sum((tf.math.exp(x)), axis=-1, keepdims=True))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xWBjt-RglVlJ",
        "colab_type": "text"
      },
      "source": [
        "Tensorflow tiene su propia implementación de la función de log softmax, `tf.nn.log_softmax`, y podemos comparar su resultado con el de nuestra `log_softmax` y veremos que el resultado es el mismo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QoDkK9iFCVxJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def relu(x):\n",
        "  return tf.math.maximum(x, 0.)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GBu8VwlnAAV",
        "colab_type": "text"
      },
      "source": [
        "De la misma manera, tensorflow cueta con la función `tf.nn.relu` y el resultado es el mismo al de nuestra función `relu`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZZGoWpCuldQ",
        "colab_type": "text"
      },
      "source": [
        "Estamos listos para multiplicar nuestros pesos por nuestras imágenes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11eGcLutMy4i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_2 = tf.linalg.matmul(t, w1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnHb4wnTbJu",
        "colab_type": "code",
        "outputId": "26702e45-1794-43af-ad05-583fec80ca09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "t_2.shape"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaiEyBu8u8ed",
        "colab_type": "text"
      },
      "source": [
        "Ahora tenemos un tensor con dimensiones `TensorShape([60000, 100])`. Es decir, tenemos un tensor rango 1 por cada una de nuestras 60,000 imágenes y cada imagen ahora tiene propiedades en cada una de nuestras 100 neuronas definidas para nuestra capa 1."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Eb_JG0c0Yk2",
        "colab_type": "text"
      },
      "source": [
        "Ahora apliquemos nuestra función `relu` a `t_2`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFm8R_wC1Hie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_2_relu = relu(t_2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MFEgB_UprrB",
        "colab_type": "text"
      },
      "source": [
        "Notar que al aplicar una función no estamos alterando la forma de nuestros tensores pues la misma función se aplica a cada elemento del tensor. La forma sigue siendo `TensorShape([60000, 100])`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VXIVapiFp91e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t_2_softmax = log_softmax(t_2_relu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vfpZ5fCv4zKU",
        "colab_type": "text"
      },
      "source": [
        "Ahora vemos que una red neuronal no es más que una función integrada por múltiples funciones; funciones compuestas. ¿Dónde está el proceso de entrenamiento? Este proceso se hace gracias al algoritmo llamado \"backpropagation\". Nos queda para la parte 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCISmJYL8Fj4",
        "colab_type": "text"
      },
      "source": [
        "## Forward pass con layers de tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzKgFlCTXenC",
        "colab_type": "text"
      },
      "source": [
        "Crearemos el forward pass para una red neuronal de dos capas fully connected lineares. Empezaremos desde 0 y construiremos hasta llegar a un modelo Sequential al estilo del famoso upper-level framework de tensorflow, Keras."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsDcfJWiXRYG",
        "colab_type": "text"
      },
      "source": [
        "### Desde (casi) cero"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ATM-IvwYV0W",
        "colab_type": "text"
      },
      "source": [
        "Comencemos de la misma manera que lo hicimos en la sesión pasada. Definamos cierta información relevante para nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zyLKTnVBXWGm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer1_units = 100\n",
        "layer2_units = 50\n",
        "n, parametros_inic = t.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31-S1ByPZH_Q",
        "colab_type": "text"
      },
      "source": [
        "esto quiere decir que nuestra primera capa tendrá 100 neuronas/unidades computacionles y la segunda 50. Nuestros datos tienen 60,000 ejemplos y 784 pixeles por imagen por lo que tenemos el mismo número de parametros"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LytVmz0sYs4m",
        "colab_type": "text"
      },
      "source": [
        "Construyamos con esta información nuestros tensores de weights: `w1` serán los weights de la primera capa y `w2` los de la segunda. Como vimos en nuestra sesión de las matemáticas del deep learning, en la primera capa `w1`, tensor de segundo rango, tendrá que multiplicar a nuestros ejemplos, tensor de segundo rango, en lo que conocemos como una multiplicación de matrices. \n",
        "\n",
        "Sabemos que nuetro tensor con las imágenes del MNIST aplanadas tiene la forma `TensorShape([60000, 784])` y queremos que nuestra primera capa tenga 100 neuronas/unidades computacionales, por lo que para realizar la multiplicación de matrices `w1` tendrá forma `TensorShape([784, 100])`. Tenemos entonces $784 * 100 = 78,400$ parámetros entrenables en nuestra primera capa. Otra forma de verlo es que cada neurona de la primera capa tiene 784 parámetros, uno para cada pixel de las imágenes.  Así, el producto entre nuestro tensor con imágenes, `t`, y `w1` tendrá `TensorShape([60,000, 100])`. \n",
        "\n",
        "Siguiendo la misma lógica, si queremos que nuestra segunda capa cuente con 50 neuronas, entonces debemos definir weights correspondientes: `w2` tendrá forma `TensorShape([100, 50])`. Por lo que el número de parámetros en la segunda capa será de $100 * 50 = 5,000$. El producto entre la activación neuronal de la primera capa de forma `TensorShape([60,000, 100])` y nuestro `w2` tendrá forma `TensorShape([100, 50])` después de la multiplicación de matrices.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JMB7g7JhTbbU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "w1 = tf.random.normal((parametros_inic, layer1_units))*0.0001\n",
        "w2 = tf.random.normal((layer1_units, layer2_units))*0.0001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kg9rtMJGTgDX",
        "colab_type": "code",
        "outputId": "8fbf77cd-9d92-4176-a293-a58124690627",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x = tf.linalg.matmul(t, w1)\n",
        "x.shape"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TensorShape([60000, 100])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDlmjsOtTxrr",
        "colab_type": "text"
      },
      "source": [
        "Como esperado, el producto de nuestra multiplicación de matrices en la primera cada tiene forma `TensorShape([60000, 100])`. Ahora aplicamos función de no linearidad ReLU para activar las neuronas de la primera capa."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie078sBiUP9k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = relu(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7wmMyoozUZGx",
        "colab_type": "text"
      },
      "source": [
        "Ahora activemos las neuronas de la segunda capa y apliquemos el softmax para obtener las predicciones que nuestro modelo arroja."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sqYh8WxSUolM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = tf.linalg.matmul(w1, w2)\n",
        "x = relu(x)\n",
        "x = log_softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cXaO-Sf1U25g",
        "colab_type": "text"
      },
      "source": [
        "Excelente, ¡tenemos la arquitectura y el forward pass de nuestra red neuronal de dos capas! Y lo hicimos (casi) desde cero. Ahora hagamos algo de \"refactoring\" para que nuestra arquitectura sea más leíble."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zupa34gpXaH2",
        "colab_type": "text"
      },
      "source": [
        "### Desde nuestras capas personalizadas (custom layers)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FJQTJXnXY68",
        "colab_type": "text"
      },
      "source": [
        "Vamos a utilizar las herramientas más comunes que utilizan los grandes deep learners. Definamos nuestra capa linear como una subclase de `tensorflow.keras.layers.Layer`, una poderosa herramienta de tensorflow que nos permite definir nuestras capas personalizadas. Cuando definimos nuestra subclase entonces esta debe encapsular un \"estado\", los weights de la layer, y el paso del tensor inicial al tensor producto. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RpjyNJo8NZU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers import Layer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuW16C7-Wzyn",
        "colab_type": "text"
      },
      "source": [
        "Al crear nuestra capa definimos la forma que tomarán los pesos de dicha capa. Como cuando creamos nuestros pesos en la subsección pasada, definimos que los pesos tendrán la forma `w_inic = tf.random.normal((parametros_pasados, neuronas))*0.0001` donde `parametros_pasados` es el número de párametos que nuestro `tensor_entrante` tiene cada una de nuestras `units` tendrá que multiplicar. En otras palabras `units` es el número de neuronas de la capa y cada una de ellas interactuará con cada uno de los parámetros de la capa anterior; recordar que estamos tratando con redes neuronales fully connected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X6KapX3Q7f37",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Linear(Layer):\n",
        "\n",
        "  def __init__(self, parametros_pasados, neuronas):\n",
        "      super(Linear, self).__init__()\n",
        "      \n",
        "      w_inic = tf.random.normal((parametros_pasados, neuronas))*0.0001\n",
        "      self.w = tf.Variable(initial_value = w_inic, dtype= \"float32\", trainable=True)\n",
        "\n",
        "  def call(self, tensor_entrante):\n",
        "      return tf.linalg.matmul(tensor_entrante, self.w)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emJ0Ael61y25",
        "colab_type": "text"
      },
      "source": [
        "Las instancias de las subclases de `tensorflow.keras.layers.Layer` tienen la propiedad de poder ser llamadas como funciones, miremos:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZpfLtKk2HNB",
        "colab_type": "text"
      },
      "source": [
        "Creamos nuestra instancia de la subclase Linear que creamos arriba y la nombramos `linear_layer`. Como queremos que represente la primera capa de la red neuronal entonces definimos que los parámetros pasados serán 784, pues tenemos 784 pixeles por cada imagen, y 100 neuronas, pues queremos que la primera capa tenga 100 neuronas. \n",
        "\n",
        "*Nota: los nombres de los argumentos de `call` e `__init__` pueden ser diferentes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0dPiGW-DDrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "linear_layer = Linear(parametros_pasados=784, neuronas=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74iRUA6QvTH0",
        "colab_type": "text"
      },
      "source": [
        "Excelente, tenemos entonces al correr `linear_layer(t)` podemos observar el mismo resultado que obtuvimos en la subsección pasada al multiplicar `tf.linalg.matmul(t, w1)`. La ventaja es que ahora podemos hacer uso de nuestra subclase `Linear` para otras capas, ya veremos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvsswgne4G6Y",
        "colab_type": "text"
      },
      "source": [
        "Con el método `trainable_variables` de nuestra instancia `linear_layer` (escribir `linear_layer.trainable_variables`) podemos observar nuestras variables, es decir, nuestros weights que serán optimizados más adelante para descubrir nuestra función que traducirá las imágenes a números."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDlNC-0F5WZt",
        "colab_type": "text"
      },
      "source": [
        "Demos el siguiente paso y creemos un modelo que utilice nuestra layer recién creada, Linear. Esta vez crearemos una subclase de la clase `tensorflow.keras.Model` y la llamaremos trivialmente `TwoLinearNeuronsAndSoftmax`. Queremos crear nuestra red neuronal de dos capas, exactamente igual a la que creamos arriba paso a paso. \n",
        "\n",
        "Las subclases de la clase Model nos permiten definir, con método `call` el forward pass, es decir el paso que llevara nuestro tensor de entrada hasta su fase final. En otras palabras, definimos el grafo que se seguirá. En la `__init__` inicializamos nuestras capas, en este caso utilizamos nuestra creada capa Linear con respectivos argumentos. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPDELFWVIJEN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import Model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AtX6I8l6IY6h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TwoLinearNeuronsAndSoftmax(Model):\n",
        "\n",
        "  def __init__(self, parametros_pasados, layer1_neuronas, layer2_neuronas):\n",
        "    super(TwoLinearNeuronsAndSoftmax, self).__init__()\n",
        "\n",
        "    self.linear1 = Linear(parametros_pasados=parametros_pasados, neuronas=layer1_neuronas)\n",
        "    self.linear2 = Linear(parametros_pasados=layer1_neuronas, neuronas=layer2_neuronas)\n",
        "    self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "\n",
        "  def call(self, inputs):\n",
        "    x = self.linear1(inputs)\n",
        "    x = relu(x)\n",
        "    x = self.linear2(x)\n",
        "    x = relu(x)\n",
        "    x = self.softmax(x)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4EkA5XA0BUy",
        "colab_type": "text"
      },
      "source": [
        "Creamos una instancia de nuestro modelo y lo llamamos simplemente `red`. Nuestro modelo comienza con 784 parámetros provenientes de las 784 neuronas que tiene cada imagen; tiene 100 neuronas en la primera layer; y 50 en la segunda layer. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCkiU8tsPDKG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red = TwoLinearNeuronsAndSoftmax(parametros_pasados=784, layer1_neuronas=100, layer2_neuronas=50)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5ctwk9sa00mo",
        "colab_type": "text"
      },
      "source": [
        "Utilizando el método `build` que todas las instancias de subclases de la clase `tensorflow.keras.Model` tienen (ya sé, es un poco escamoso pero vale la pena que lo repitamos), podemos entonces activar nuestro modelo. Como argumentos indicamos la forma del tensor que ingresaremos, el tensor `t` con nuestros datos; como tenemos 60,000 imágenes con 784 pixeles cada una entonces la forma que ingreamos es (60000, 784). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qxENTBnZ54zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red.build((60000,784))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Xf-C1F616J0",
        "colab_type": "text"
      },
      "source": [
        "Una vez construido nuestro modelo `red` podemos utilizar su método `summary` para recibir un resumen de lo que abarca nuestro modelo."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B7awdr-u6Mft",
        "colab_type": "code",
        "outputId": "d9fd161b-da56-440e-f6fc-501c5942e400",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "red.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"two_linear_neurons_and_softmax\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "linear_1 (Linear)            multiple                  78400     \n",
            "_________________________________________________________________\n",
            "linear_2 (Linear)            multiple                  5000      \n",
            "_________________________________________________________________\n",
            "softmax (Softmax)            multiple                  0         \n",
            "=================================================================\n",
            "Total params: 83,400\n",
            "Trainable params: 83,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6T6fCElFlDs",
        "colab_type": "text"
      },
      "source": [
        "Llamamos a nuestro modelo sobre nuestro tensor de imágenes `t`. Y la forma es la que esparabamos (60000, 50)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbf5-8jSRFN9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "22780b09-a122-44a4-de3a-a4d12c49da0c"
      },
      "source": [
        "red(t)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=145, shape=(60000, 50), dtype=float32, numpy=\n",
              "array([[0.01999966, 0.01999891, 0.01999891, ..., 0.01999891, 0.01999891,\n",
              "        0.01999891],\n",
              "       [0.01999917, 0.01999875, 0.01999954, ..., 0.01999875, 0.01999875,\n",
              "        0.02000156],\n",
              "       [0.02000059, 0.01999874, 0.02000078, ..., 0.01999874, 0.0200033 ,\n",
              "        0.02000029],\n",
              "       ...,\n",
              "       [0.02000185, 0.01999897, 0.01999897, ..., 0.01999897, 0.0200012 ,\n",
              "        0.01999897],\n",
              "       [0.01999901, 0.01999896, 0.02000101, ..., 0.01999896, 0.01999896,\n",
              "        0.02000188],\n",
              "       [0.01999903, 0.0199989 , 0.02000056, ..., 0.0199989 , 0.02000131,\n",
              "        0.0199989 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "At9xbGKQF7dE",
        "colab_type": "text"
      },
      "source": [
        "### Desde Sequential "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EYWbfs4CSVyi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "red = tf.keras.Sequential([Linear(parametros_pasados=784, neuronas=100),\n",
        "                           tf.keras.layers.ReLU(),\n",
        "                           Linear(parametros_pasados=100, neuronas=50),\n",
        "                           tf.keras.layers.ReLU(),\n",
        "                           tf.keras.layers.Softmax(axis=-1)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nZit0GoW-xc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "9906d00b-0b98-42f6-e7b9-229e305be9a7"
      },
      "source": [
        "red.summary()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "linear_3 (Linear)            multiple                  78400     \n",
            "_________________________________________________________________\n",
            "re_lu (ReLU)                 multiple                  0         \n",
            "_________________________________________________________________\n",
            "linear_4 (Linear)            multiple                  5000      \n",
            "_________________________________________________________________\n",
            "re_lu_1 (ReLU)               multiple                  0         \n",
            "_________________________________________________________________\n",
            "softmax_1 (Softmax)          multiple                  0         \n",
            "=================================================================\n",
            "Total params: 83,400\n",
            "Trainable params: 83,400\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AIFjToGrGL4e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        },
        "outputId": "df0d4219-9623-4ec3-d3e1-1dd637a3d38c"
      },
      "source": [
        "red(t)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: id=180, shape=(60000, 50), dtype=float32, numpy=\n",
              "array([[0.01999853, 0.0199994 , 0.02001062, ..., 0.01999963, 0.01999853,\n",
              "        0.01999853],\n",
              "       [0.01999868, 0.02000212, 0.02000568, ..., 0.01999868, 0.01999868,\n",
              "        0.01999868],\n",
              "       [0.01999903, 0.02000041, 0.02000166, ..., 0.02000176, 0.01999853,\n",
              "        0.01999853],\n",
              "       ...,\n",
              "       [0.01999856, 0.02000054, 0.02000801, ..., 0.02000514, 0.01999856,\n",
              "        0.01999856],\n",
              "       [0.01999891, 0.01999962, 0.02000536, ..., 0.01999993, 0.01999891,\n",
              "        0.01999891],\n",
              "       [0.02000081, 0.0199985 , 0.02000759, ..., 0.02000002, 0.02000046,\n",
              "        0.0199985 ]], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4yGDI3bp8_eD",
        "colab_type": "text"
      },
      "source": [
        "## Backward pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XejvOSkp9DB4",
        "colab_type": "text"
      },
      "source": [
        "You take the gradient of a tensor to help you figure out what you need to do to minimize error. [Cami Williams de Fcebook](https://medium.com/@cwillycs/committing-to-pytorch-by-someone-who-doesnt-know-a-ton-about-pytorch-fa222253cf2d).\n",
        "\n",
        "Gradient descent is an algorithm that allows us to minimize error efficiently. The error is determined by our data. We have data that is properly classified and improperly classified. We take the gradient to decrease the number of improperly classified items."
      ]
    }
  ]
}